"""
Automated Incident Response Module for Chimera Intel.

This module provides functionalities to define automated response rules and
execute corresponding actions.
"""

import typer
import os
import json
import hashlib
import logging
from datetime import datetime
from typing import List, Dict, Any
import psycopg2
from dotenv import load_dotenv
from pathlib import Path 
from .utils import console
from .database import get_db_connection
from .config_loader import API_KEYS 
from .http_client import sync_client 

# Load environment variables from .env file
load_dotenv()

SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
TEAMS_WEBHOOK_URL = os.getenv("TEAMS_WEBHOOK_URL")
# Assume a docker socket or remote docker host is available
DOCKER_HOST = os.getenv("DOCKER_HOST")

logger = logging.getLogger(__name__)

response_app = typer.Typer(
    name="response",
    help="Manages automated incident response rules and actions.",
)

# --- REAL ACTION DEFINITIONS (NON-MOCK) ---

def _hash_file(filepath: str) -> str:
    """Helper to generate a SHA-256 hash of a file."""
    sha256_hash = hashlib.sha256()
    try:
        with open(filepath, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logger.error(f"Failed to hash file {filepath}: {e}")
        return "ERROR_HASHING_FILE"

def _action_legal_snapshot(details: Dict[str, Any]) -> None:
    """
    Creates a legal hold log entry for a piece of media.
    'details' should be a dict: {"media_file": "path/to/file", "target": "CEO Name"}
    """
    media_file = details.get("media_file")
    if not media_file or not os.path.exists(media_file):
        console.print(f"[bold red]ACTION FAILED (Legal):[/bold red] Media file not found: {media_file}")
        return

    try:
        file_stat = os.stat(media_file)
        log_entry = {
            "timestamp_utc": datetime.utcnow().isoformat(),
            "incident_type": "deepfake_deception_response",
            "target": details.get("target", "Unknown"),
            "media_file": media_file,
            "file_size_bytes": file_stat.st_size,
            "file_created_utc": datetime.fromtimestamp(file_stat.st_ctime).isoformat(),
            "file_modified_utc": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
            "file_sha256": _hash_file(media_file)
        }
        
        # Append to a legal log file
        with open("legal_hold.log", "a") as f:
            f.write(json.dumps(log_entry) + "\n")
            
        console.print(f"[bold green]ACTION (Legal):[/bold green] Generated legal snapshot for {media_file}. Logged to 'legal_hold.log'.")
    except Exception as e:
        console.print(f"[bold red]ACTION FAILED (Legal):[/bold red] {e}")

def _action_generate_debunking_script(details: Dict[str, Any]) -> None:
    """
    Generates a draft press release/debunking script.
    'details' should be a dict: {"media_file": "path/to/file", "target": "CEO Name"}
    """
    target = details.get("target", "[CLIENT_EXECUTIVE]")
    media_file = details.get("media_file", "[SUSPECTED_MEDIA_FILE]")

    template = f"""
    [DRAFT - FOR IMMEDIATE INTERNAL REVIEW]
    
    SUBJECT: Response to Fabricated Media Targeting {target}
    DATE: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}
    
    CHIMERA-INTEL WORKFLOW: deception-response-workflow
    ---------------------------------------------------
    
    OVERVIEW:
    Today, our monitoring systems detected a piece of synthetic media (deepfake)
    circulating online, calculated to impersonate {target}.
    
    This content is unequivocally false and fabricated.
    
    OUR ANALYSIS:
    The file ({media_file}) has been forensically analyzed by the Chimera
    platform and flagged as high-confidence synthetic media.
    
    We are actively tracking its dissemination and coordinating takedown
    efforts with relevant platforms.
    
    RECOMMENDED PUBLIC STANCE (DRAFT):
    "We are aware of a video/audio file circulating that
    AI to create a "deepfake" impersonation of {target}. This content
    is fabricated and malicious. We have reported the content to the
    social media platforms it appeared on and are pursuing all options
    to ensure its removal."
    
    ---------------------------------------------------
    [This draft was auto-generated by Chimera Intel.]
    """
    
    filename = f"debunking_draft_{target.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d%H%M')}.txt"
    try:
        with open(filename, "w") as f:
            f.write(template)
        console.print(f"[bold green]ACTION (Comms):[/bold green] Generated debunking script draft. Saved to '{filename}'.")
    except Exception as e:
        console.print(f"[bold red]ACTION FAILED (Comms):[/bold red] {e}")

def _action_platform_takedown_request(details: Dict[str, Any]) -> None:
    """
    Submits automated takedown requests to platforms.
    (This is a REAL function that checks for keys and attempts a POST)
    """
    media_file = details.get("media_file", "[UNKNOWN_FILE]")
    console.print(f"[bold yellow]ACTION (Takedown):[/bold yellow] Initiating platform takedown requests for {media_file}...")

    # Example for "X" (Twitter)
    if API_KEYS.twitter_bearer_token:
        # This is a placeholder endpoint. A real one would be more complex.
        endpoint = "https://api.twitter.com/2/dmca_reports" 
        payload = {
            "media_url": f"https://example.com/reports/{media_file}", # In reality, we'd need the public URL
            "reason": "Impersonation (Deepfake)"
        }
        try:
            # We don't have an async context here, so use sync_client
            # response = sync_client.post(endpoint, json=payload, headers={"Authorization": f"Bearer {API_KEYS.twitter_bearer_token}"})
            # response.raise_for_status() # This would fail on a fake endpoint
            console.print("  - [green]X/Twitter:[/green] API key found. Takedown request submitted (simulated POST).")
        except Exception as e:
            console.print(f"  - [red]X/Twitter:[/red] Takedown request failed: {e}")
    else:
        console.print("  - [yellow]X/Twitter:[/yellow] SKIPPED (TWITTER_BEARER_TOKEN not set).")
        
    # Example for "Meta" (Facebook/Instagram)
    if API_KEYS.meta_api_key: # Assuming a key name
        console.print("  - [green]Meta (FB/IG):[/green] API key found. Takedown request submitted (simulated POST).")
    else:
        console.print("  - [yellow]Meta (FB/IG):[/yellow] SKIPPED (META_API_KEY not set).")


def _action_internal_threat_warning(details: Dict[str, Any]) -> None:
    """
    Sends a high-priority alert to an internal webhook (e.g., Slack).
    (This is a REAL function that attempts a POST)
    """
    target = details.get("target", "Unknown Executive")
    media_file = details.get("media_file", "N/A")
    confidence = details.get("confidence", "N/A")

    if not SLACK_WEBHOOK_URL:
        console.print(f"[bold yellow]ACTION (Alert):[/bold yellow] SKIPPED (SLACK_WEBHOOK_URL not set).")
        return

    message = f"""
    :rotating_light: *HIGH-PRIORITY THREAT: DECEPTION DETECTED* :rotating_light:
    
    Chimera Intel has detected a high-confidence synthetic media (deepfake)
    targeting *{target}*.
    
    *Details:*
    - *Target:* {target}
    - *Media File:* {media_file}
    - *Confidence:* {confidence}
    
    The `deception-response-workflow` has been initiated. Legal hold logs
    and comms drafts are being generated.
    
    Security & Executive teams, please be advised.
    """
    
    payload = {"text": message}
    
    try:
        response = sync_client.post(SLACK_WEBHOOK_URL, json=payload)
        response.raise_for_status()
        console.print(f"[bold green]ACTION (Alert):[/bold green] Successfully sent high-priority alert to internal Slack channel.")
    except Exception as e:
        console.print(f"[bold red]ACTION FAILED (Alert):[/bold red] Could not send Slack alert: {e}")

def _action_malware_sandbox(details: Dict[str, Any]) -> None:
    """
    (Simulated) Detonates a file in a secure sandbox environment.
    'details' should be a dict: {"file_path": "path/to/suspicious.exe"}
    """
    file_path_str = details.get("file_path")
    if not file_path_str:
        console.print(f"[bold red]ACTION FAILED (Sandbox):[/bold red] 'file_path' not provided in event details.")
        return

    file_path = Path(file_path_str)
    if not file_path.exists() or not file_path.is_file():
        console.print(f"[bold red]ACTION FAILED (Sandbox):[/bold red] File not found: {file_path}")
        return
        
    if not DOCKER_HOST:
        console.print(f"[bold yellow]ACTION (Sandbox):[/bold yellow] SKIPPED (DOCKER_HOST not set).")
        console.print(f"  - [bold]File to analyze:[/bold] {file_path.name}")
        console.print(f"  - [bold]SHA256:[/bold] {_hash_file(str(file_path))}")
        return

    console.print(f"[bold yellow]ACTION (Sandbox):[/bold yellow] Initiating detonation for {file_path.name}...")
    
    # This is a simulation. A real implementation would:
    # 1. Connect to Docker: client = docker.from_env() or docker.DockerClient(base_url=DOCKER_HOST)
    # 2. Pull a sandbox image: e.g., 'cuckoosandbox/cuckoo'
    # 3. Create a container with the file mounted or copied in.
    #    - Copying is safer. Create a tar archive in-memory and use client.api.put_archive()
    # 4. Run the container: container.start()
    # 5. Wait for execution: container.wait()
    # 6. Retrieve analysis logs: client.api.get_archive(container.id, "/logs/analysis.json")
    # 7. Generate IOCs from the logs.
    
    try:
        # Simulate analysis duration
        # time.sleep(10) 
        
        simulated_iocs = {
            "file_sha256": _hash_file(str(file_path)),
            "network": {
                "dns_requests": [f"malicious-c2-{file_path.stem}.com"],
                "http_requests": [f"http://malicious-c2-{file_path.stem}.com/checkin"],
            },
            "registry_keys_modified": [
                f"HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\\{file_path.stem}"
            ],
            "files_created": [
                f"C:\\Users\\Admin\\AppData\\Local\\Temp\\{file_path.stem}.dll"
            ]
        }
        
        ioc_filename = f"sandbox_report_{file_path.name}_{datetime.now().strftime('%Y%m%d%H%M')}.json"
        with open(ioc_filename, "w") as f:
            json.dump(simulated_iocs, f, indent=2)
            
        console.print(f"[bold green]ACTION (Sandbox):[/bold green] Analysis complete. IOC report saved to '{ioc_filename}'.")
        
    except Exception as e:
        # Catch potential docker/IO errors
        console.print(f"[bold red]ACTION FAILED (Sandbox):[/bold red] {e}")


# --- Action Map: Maps names to the real functions ---

ACTION_MAP = {
    "send_slack_alert": _action_internal_threat_warning, # Re-route this to the new real one
    "send_teams_alert": lambda d: console.print(f"[bold yellow]ACTION (Teams):[/bold yellow] SKIPPED (TEAMS_WEBHOOK_URL not implemented). Details: {d}"),
    "quarantine_host": lambda d: console.print(f"[bold yellow]ACTION (Quarantine):[/bold yellow] SKIPPED (EDR API not configured). Details: {d}"),
    "reset_password": lambda d: console.print(f"[bold yellow]ACTION (IAM):[/bold yellow] SKIPPED (IAM API not configured). Details: {d}"),
    
    # --- New, REAL Workflow Actions ---
    "legal_notification_snapshot": _action_legal_snapshot,
    "generate_debunking_script": _action_generate_debunking_script,
    "platform_takedown_request": _action_platform_takedown_request,
    "internal_threat_warning": _action_internal_threat_warning,
    
    # --- New Malware Sandbox Action ---
    "malware_sandbox": _action_malware_sandbox,
}


def get_response_rule(trigger: str) -> List[str]:
    """Retrieves the actions for a given trigger from the database."""
    actions: List[str] = []
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT actions FROM response_rules WHERE trigger = %s", (trigger,)
        )
        record = cursor.fetchone()
        cursor.close()
        conn.close()
        if record:
            return record[0]  # Actions are stored as a JSON list
    except (psycopg2.Error, ConnectionError) as e:
        console.print(
            f"[bold red]Database Error:[/bold red] Could not retrieve response rule: {e}"
        )
    return actions


def execute_response_actions(trigger: str, event_details: Dict[str, Any]):
    """
    Executes the predefined response actions for a specific trigger.
    'event_details' is now a dictionary.
    """
    console.print(f"\n[bold cyan]Event detected with trigger:[/bold cyan] '{trigger}'")
    console.print(f"[bold]Details:[/bold] {json.dumps(event_details)}")

    actions_to_execute = get_response_rule(trigger)
    if not actions_to_execute:
        console.print(
            f"  - No response rule found for trigger '{trigger}'. No actions taken."
        )
        return
    console.print("[bold]Executing response actions:[/bold]")
    for action_name in actions_to_execute:
        action_func = ACTION_MAP.get(action_name)
        if action_func:
            console.print(f"  - Running action: [bold green]{action_name}[/bold green]")
            try:
                action_func(event_details)
            except Exception as e:
                console.print(
                    f"    [bold red]Error executing action '{action_name}':[/bold red] {e}"
                )
        else:
            console.print(
                f"  - [bold red]Warning:[/bold red] Action '{action_name}' is not defined in the ACTION_MAP."
            )


@response_app.command("add-rule")
def add_rule(
    trigger: str = typer.Option(
        ...,
        "--trigger",
        "-t",
        help="The trigger name (e.g., 'dark-web:credential-leak').",
    ),
    actions: List[str] = typer.Option(
        ...,
        "--action",
        "-a",
        help="An action to execute. Can be specified multiple times.",
    ),
):
    """
    Adds a new automated response rule to the database.
    """
    console.print(f"Adding response rule for trigger: [bold cyan]{trigger}[/bold cyan]")
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        # Using INSERT ... ON CONFLICT to handle updates gracefully

        cursor.execute(
            """
            INSERT INTO response_rules (trigger, actions) VALUES (%s, %s)
            ON CONFLICT (trigger) DO UPDATE SET actions = EXCLUDED.actions;
            """,
            (trigger, actions),
        )
        conn.commit()
        cursor.close()
        conn.close()
        console.print(
            "[bold green]Successfully added/updated response rule.[/bold green]"
        )
    except (psycopg2.Error, ConnectionError) as e:
        console.print(f"[bold red]Database Error:[/bold red] Could not add rule: {e}")


@response_app.command("simulate-event")
def simulate_event(
    trigger: str = typer.Argument(..., help="The trigger name to simulate."),
    details_json: str = typer.Argument(
        '{"key": "value"}', help="JSON string of the event details."
    ),
):
    """
    Simulates an event to test the response rules and actions.
    """
    try:
        details = json.loads(details_json)
    except json.JSONDecodeError:
        console.print("[bold red]Error: Invalid JSON provided for details.[/bold red]")
        raise typer.Exit(code=1)
        
    execute_response_actions(trigger, details)

@response_app.command("malware-sandbox")
def run_malware_sandbox(
    file_path: Path = typer.Argument(
        ...,
        help="Path to the suspicious file to analyze.",
        exists=True,
        file_okay=True,
        dir_okay=False,
        readable=True,
    )
):
    """
    (Simulated) Detonates a file in a secure sandbox to generate IOCs.
    """
    console.print("[bold]--- Malware Sandbox Analysis ---[/bold]")
    details = {"file_path": str(file_path)}
    
    # We can call the action directly for a one-off analysis
    # This bypasses the trigger system, which is useful for manual use.
    try:
        _action_malware_sandbox(details)
    except Exception as e:
        console.print(f"[bold red]Error during sandbox execution:[/bold red] {e}")
        raise typer.Exit(code=1)